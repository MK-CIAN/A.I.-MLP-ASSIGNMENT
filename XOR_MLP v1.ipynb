{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22/11/2019\n",
    "\n",
    "The code here is close to Nielsen. Each activation is treated as a column vector, even the last one which for XOR is just a simple number and is encloded in a shape (1,1) column vector of just one row, i.e if activation value of output neuron is a, then it is computed as np.array([[a]]).\n",
    "\n",
    "Can easily adapt code here for the MLP excercises and the Iris classification problem.\n",
    "But you may need to use more than 2 hidden neurons and more than 1 output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm(z):\n",
    "    return  1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigm_deriv(z):\n",
    "    a = sigm(z)\n",
    "    return a*(1 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR_MLP:\n",
    "    def __init__(self):\n",
    "        self.train_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "        self.train_outputs = np.array([0,1,1,0])\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of 2 neurons\n",
    "        self.w2 = np.random.randn(2,2)\n",
    "        self.b2 = np.random.randn(2,1)\n",
    "        \n",
    "        # output layer has 1 neuron\n",
    "        self.w3 = np.random.randn(1,2)\n",
    "        self.b3 = np.random.randn(1,1)\n",
    "        \n",
    "\n",
    "    def feedforward(self, xs):    \n",
    "        # here xs is a matrix where each column is an input vector\n",
    "        # w2.dot(xs) applies the weight matrix w2 to each input at once\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)            \n",
    "        return a3s\n",
    "\n",
    "    \n",
    "    def backprop(self, xs, ys):   # Assumed here that input vectors are rows in xs\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x,y in zip(xs,ys):               # for zip to work, each x in xs must be a row vector\n",
    "            a1 = x.reshape(2,1)              # convert input row vector x into (2,1) column vector\n",
    "            z2 = self.w2.dot(a1) + self.b2   # so will z2 and a2\n",
    "            a2 = sigm(z2)                    # column vector shape (2,1)\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3   # a simple number in a (1,1) column vector\n",
    "            a3 = sigm(z3)                    # so is a3\n",
    "            \n",
    "            delta3 = (a3-y) * sigm_deriv(z3)                   # delta3.shape is (1,1)\n",
    "           \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))  # w3 shape is (1,2), w3.T shape is (2,1)\n",
    "                                                               # delta2 is shape (2,1)\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)  # shape (1,1) by (1,2) gives (1,2)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)  # shape (2,1) by (1,2) gives (2,2)\n",
    "        \n",
    "           \n",
    "            cost += ((a3 - y)**2).sum() \n",
    "        \n",
    "        n = len(ys)  # number of training vectors    \n",
    "        \n",
    "        # get the average change per training input  \n",
    "        # return the average adjustments to the biases and weights \n",
    "        # in each layer and the cost\n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "        \n",
    "    def train(self, epochs, eta):\n",
    "        xs = self.train_inputs\n",
    "        ys = self.train_outputs\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(xs,ys)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        return cost\n",
    "                \n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor = XOR_MLP()\n",
    "xs = xor.train_inputs.T\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "epochs = 1000\n",
    "c = xor.train(epochs, 3.0)\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "fig, axs = plt.subplots(3,1,figsize=(10,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, c)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis[:61], c[:61])\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis[900:], c[900:])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: copy and adapt the above XOR_MLP code so that it uses 3 neurons in the hidden layer. Train such a MLP and see if it learns faster than the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "class XOR_MLPv2:\n",
    "    def __init__(self):\n",
    "        self.train_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "        self.train_outputs = np.array([0,1,1,0])\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of 3 neurons\n",
    "        self.w2 = np.random.randn(3,2)\n",
    "        self.b2 = np.random.randn(3,1)\n",
    "        \n",
    "        # output layer has 1 neuron\n",
    "        self.w3 = np.random.randn(1,3)\n",
    "        self.b3 = np.random.randn(1,1)\n",
    "        \n",
    "\n",
    "    def feedforward(self, xs):    \n",
    "        # here xs is a matrix where each column is an input vector\n",
    "        # w2.dot(xs) applies the weight matrix w2 to each input at once\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)            \n",
    "        return a3s\n",
    "\n",
    "    \n",
    "    def backprop(self, xs, ys):   # Assumed here that input vectors are rows in xs\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x,y in zip(xs,ys):               # for zip to work, each x in xs must be a row vector\n",
    "            a1 = x.reshape(2,1)              # convert input row vector x into (2,1) column vector\n",
    "            z2 = self.w2.dot(a1) + self.b2   # so will z2 and a2\n",
    "            a2 = sigm(z2)                    # column vector shape (2,1)\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3   # a simple number in a (1,1) column vector\n",
    "            a3 = sigm(z3)                    # so is a3\n",
    "            \n",
    "            delta3 = (a3-y) * sigm_deriv(z3)                   # delta3.shape is (1,1)\n",
    "           \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))  # w3 shape is (1,2), w3.T shape is (2,1)\n",
    "                                                               # delta2 is shape (2,1)\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)  # shape (1,1) by (1,2) gives (1,2)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)  # shape (2,1) by (1,2) gives (2,2)\n",
    "        \n",
    "           \n",
    "            cost += ((a3 - y)**2).sum() \n",
    "        \n",
    "        n = len(ys)  # number of training vectors    \n",
    "        \n",
    "        # get the average change per training input  \n",
    "        # return the average adjustments to the biases and weights \n",
    "        # in each layer and the cost\n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "        \n",
    "    def train(self, epochs, eta):\n",
    "        xs = self.train_inputs\n",
    "        ys = self.train_outputs\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(xs,ys)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        return cost\n",
    "                \n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing xor_mlpv2\n",
    "xor2 = XOR_MLPv2()\n",
    "xs = xor2.train_inputs.T\n",
    "\n",
    "print(xor2.feedforward(xs))\n",
    "\n",
    "epochs = 1000\n",
    "c = xor2.train(epochs, 3.0)\n",
    "\n",
    "print(xor2.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "fig, axs = plt.subplots(3,1,figsize=(10,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, c)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis[:61], c[:61])\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis[900:], c[900:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more general purpose MLP with m input neurons, n hidden neurons and o output neurond\n",
    "# You must complete this code yourself\n",
    "class MLP:\n",
    "    def __init__(self, m, n, o):\n",
    "\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.o = o\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of N neurons\n",
    "        self.w2 = np.random.randn(n,m)\n",
    "        self.b2 = np.random.randn(n,1)\n",
    "        \n",
    "        # output layer has O neurons but code is incorrect\n",
    "        # code here needs to be modified\n",
    "        self.w3 = np.random.randn(o,n)\n",
    "        self.b3 = np.random.randn(o,1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def feedforward(self, xs):    \n",
    "        # here xs is a matrix where each column is an input vector\n",
    "        # w2.dot(xs) applies the weight matrix w2 to each input at once\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)   \n",
    "        # Format the output for better readability\n",
    "        formatted_output = self.format_output(a3s)\n",
    "\n",
    "        return formatted_output         \n",
    "    \n",
    "\n",
    "    \n",
    "    def backprop(self, xs, ys):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x, y in zip(xs,ys):            \n",
    "            a1 = x.reshape(self.m, 1)        # convert input vector x into (2,1) column vector\n",
    "            y = y.reshape(self.o, 1)         # convert output vector y into (1,1) column vector\n",
    "\n",
    "\n",
    "            z2 = self.w2.dot(a1) + self.b2   # so will z2 and a2\n",
    "            a2 = sigm(z2)                    # column vector shape (2,1)\n",
    "            a2 = a2.reshape(self.n, 1)       # convert a2 into (2,1) column vector\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3   # a simple number in a (1,1) column vector\n",
    "            a3 = sigm(z3)                    # so is a3\n",
    "            a3 = a3.reshape(self.o, 1)       # convert a3 into (1,1) column vector\n",
    "\n",
    "            delta3 = (a3-y) * sigm_deriv(z3)                   # delta3.shape is (1,1)\n",
    "           \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))  # w3 shape is (1,2), w3.T shape is (2,1)\n",
    "                                                               # delta2 is shape (2,1)\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)  # shape (1,1) by (1,2) gives (1,2)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)  # shape (2,1) by (1,2) gives (2,2)\n",
    "        \n",
    "           \n",
    "            cost += ((a3 - y)**2).sum() \n",
    "        \n",
    "        n = len(ys)  # number of training vectors    \n",
    "        \n",
    "        # get the average change per training input  \n",
    "        # return the average adjustments to the biases and weights \n",
    "        # in each layer and the cost\n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        return self.feedforward(xs)\n",
    "        \n",
    "    def train(self, epochs, eta):\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(self.train_inputs ,self.train_outputs)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        plt.title('Loss Per Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        return cost\n",
    "                \n",
    "    def format_output(self, output):\n",
    "        # Format the output for better readability\n",
    "        formatted = [', '.join(f'{value:.4f}' for value in row) for row in output.T]\n",
    "        return '\\n'.join(formatted)\n",
    "    \n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP With Cross Entropy Cost\n",
    "class MLP_Cross_Entropy:\n",
    "    def __init__(self, m, n, o):\n",
    "\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.o = o\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of N neurons\n",
    "        self.w2 = np.random.randn(n,m)\n",
    "        self.b2 = np.random.randn(n,1)\n",
    "        \n",
    "        self.w3 = np.random.randn(o,n)\n",
    "        self.b3 = np.random.randn(o,1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def feedforward(self, xs):    \n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)   \n",
    "        # Format the output for better readability\n",
    "        formatted_output = self.format_output(a3s)\n",
    "        return formatted_output         \n",
    "    \n",
    "\n",
    "    \n",
    "    def backprop(self, xs, ys):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x, y in zip(xs,ys):            \n",
    "            a1 = x.reshape(self.m, 1)        \n",
    "            y = y.reshape(self.o, 1)         \n",
    "\n",
    "\n",
    "            z2 = self.w2.dot(a1) + self.b2   \n",
    "            a2 = sigm(z2)                    \n",
    "            a2 = a2.reshape(self.n, 1)       \n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3  \n",
    "            a3 = sigm(z3)                    \n",
    "            a3 = a3.reshape(self.o, 1)       \n",
    "\n",
    "            #Adding Cross Entropy Cost Function\n",
    "            delta3 = (a3-y)                 \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3)) \n",
    "                                                            \n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)  \n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)  \n",
    "        \n",
    "           \n",
    "            cost += ((a3 - y)**2).sum() \n",
    "        \n",
    "        n = len(ys)  # number of training vectors    \n",
    "        \n",
    "        # get the average change per training input  \n",
    "        # return the average adjustments to the biases and weights \n",
    "        # in each layer and the cost\n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        return self.feedforward(xs)\n",
    "        \n",
    "    def train(self, epochs, eta):\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(self.train_inputs ,self.train_outputs)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        plt.title('Loss Per Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        return cost\n",
    "                \n",
    "    def format_output(self, output):\n",
    "        # Format the output for better readability\n",
    "        formatted = [', '.join(f'{value:.4f}' for value in row) for row in output.T]\n",
    "        return '\\n'.join(formatted)\n",
    "    \n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1\n",
    "#Testing MLP\n",
    "prob1_mlp = MLP(3,4,1)\n",
    "prob1_mlp.train_inputs = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])\n",
    "prob1_mlp.train_outputs = np.array([0,1,1,0])\n",
    "\n",
    "xs = prob1_mlp.train_inputs.T\n",
    "\n",
    "print(\"Inputs Before Training:\\n\" + prob1_mlp.feedforward(xs))\n",
    "\n",
    "\n",
    "#As mentoined in doc starting with 2000 epochs and changing learning rate as needed\n",
    "epochs = 2000\n",
    "learning_rate = 10.0\n",
    "\n",
    "c = prob1_mlp.train(epochs, learning_rate)\n",
    "\n",
    "print(\"\\nInputs After Training:\\n\" + prob1_mlp.feedforward(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2 Neural Network with 3 input and 2 output\n",
    "prob2_mlp = MLP(3,4,2)\n",
    "\n",
    "prob2_mlp.train_inputs = np.array([[1,1,0], [1, -1, -1], [-1, 1, 1], [-1, -1, 1], [0, 1, -1], [0, -1, -1], [1, 1, 1]])\n",
    "\n",
    "prob2_mlp.train_outputs = np.array([[1, 0], [0, 1], [1, 1], [1, 0], [1, 0], [1, 1], [1, 1]])\n",
    "\n",
    "xs = prob2_mlp.train_inputs.T\n",
    "\n",
    "print(\"Inputs Before Training:\\n\" + prob2_mlp.feedforward(xs))\n",
    "\n",
    "epochs = 2000\n",
    "learning_rate = 16.0\n",
    "\n",
    "c = prob2_mlp.train(epochs, learning_rate)\n",
    "print(\"\\nInputs After Training:\\n\" + prob2_mlp.feedforward(xs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3 Transport Choice\n",
    "#Training Input data to see what mode of transport a user must use:\n",
    "#Gender\n",
    "# 0 - Male, 1 - Female\n",
    "#Car Ownership\n",
    "# 0, 1 or 2 Car Ownership is a quantitative integer\n",
    "#Travel Costs\n",
    "# 0 - Cheap, 1 - Standard, 2 - Expensive\n",
    "#Income\n",
    "# 0 - Low, 1 - Medium, 2 - High\n",
    "\n",
    "#Possible Outputs\n",
    "# Bus [1, 0, 0], Train [0, 1, 0], Car [0, 0, 1]\n",
    "\n",
    "prob3_mlp = MLP(4,6,3)\n",
    "\n",
    "prob3_mlp.train_inputs = np.array([[0, 0, 0, 0], [0, 1, 0, 1], [1, 1, 0 ,1], [1, 0, 0, 0], [0, 1, 0, 1], [0, 0, 1 ,1], [1, 1, 1, 1], [1, 1, 2, 2], [0, 2, 2, 1], [1, 2, 2 ,2]])\n",
    "prob3_mlp.train_outputs = np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1]])\n",
    "\n",
    "#Outputs before training\n",
    "print(\"Outputs before training:\")\n",
    "xs = prob3_mlp.train_inputs.T\n",
    "print(prob3_mlp.feedforward(xs))\n",
    "\n",
    "epochs = 1000\n",
    "learning_rate = 11.0\n",
    "\n",
    "print(\"\\nOutputs after training:\")\n",
    "c = prob3_mlp.train(epochs, learning_rate)\n",
    "print(\"cost = \", str(c[-1]))\n",
    "print(prob3_mlp.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "\n",
    "#Test case for a Women with no cars, expensive travel costs and medium income\n",
    "print(\"\\nWomen Test Case:\")\n",
    "test_case = np.array([1, 0, 2, 1]).reshape(4,1)\n",
    "print(prob3_mlp.predict(test_case))\n",
    "\n",
    "\n",
    "#Copying the data to a dataframe and saving it to a csv file seperated by commas\n",
    "prob3_mlp_df = pd.DataFrame(prob3_mlp.train_inputs)\n",
    "prob3_mlp_df.to_csv('transport.csv', sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probelem 4 Bringing in Iris Dataset and cleaning it\n",
    "#Reading the data from the csv file\n",
    "iris_data = pd.read_csv('iris_data.csv', header=None)\n",
    "\n",
    "#Iris data has 4 features and 1 output, the last column is the output\n",
    "iris_data_output = iris_data.iloc[:, -1]\n",
    "\n",
    "#Converting the output to a one hot encoded vector\n",
    "iris_data_output = pd.get_dummies(iris_data_output)\n",
    "\n",
    "#Converting the dataframe to a numpy array\n",
    "iris_data_output = iris_data_output.to_numpy()\n",
    "\n",
    "#Converting the output to 1's and 0's\n",
    "iris_data_output = iris_data_output.astype(int)\n",
    "\n",
    "#Dropping the last column from the dataframe\n",
    "iris_data = iris_data.drop(iris_data.columns[[-1]], axis=1)\n",
    "\n",
    "#Converting the dataframe to a numpy array\n",
    "iris_data_inputs = iris_data.to_numpy()\n",
    "\n",
    "#Ensuring all the remaining values are floats\n",
    "iris_data_inputs = iris_data_inputs.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iris data set with MLP\n",
    "xs = iris_data_inputs.T\n",
    "\n",
    "#Creating the MLP\n",
    "iris_mlp = MLP(4,7,3)\n",
    "\n",
    "#Setting the training inputs and outputs\n",
    "iris_mlp.train_inputs = iris_data_inputs\n",
    "iris_mlp.train_outputs = iris_data_output\n",
    "\n",
    "#Outputs before training\n",
    "print(\"Outputs before training:\\n\" + iris_mlp.feedforward(xs))\n",
    "\n",
    "#Training the MLP\n",
    "epochs = 1000\n",
    "learning_rate = 0.8\n",
    "\n",
    "print(\"\\nOutputs after training:\")\n",
    "c = iris_mlp.train(epochs, learning_rate)\n",
    "print(\"cost = \", str(c[-1]))\n",
    "print(iris_mlp.feedforward(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4 Iris Dataset with Cross Entropy Cost\n",
    "#Creating the MLP\n",
    "iris_mlp_cross_entropy = MLP_Cross_Entropy(4,7,3)\n",
    "\n",
    "#Setting the training inputs and outputs\n",
    "iris_mlp_cross_entropy.train_inputs = iris_data_inputs\n",
    "iris_mlp_cross_entropy.train_outputs = iris_data_output\n",
    "\n",
    "#Training the MLP\n",
    "epochs = 1000\n",
    "learning_rate = 0.17\n",
    "\n",
    "print(\"\\nOutputs with Cross Entropy after training:\")\n",
    "c = iris_mlp_cross_entropy.train(epochs, learning_rate)\n",
    "print(\"cost = \", str(c[-1]))\n",
    "print(iris_mlp_cross_entropy.feedforward(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bringing in Heart Disease Dataset and cleaning it\n",
    "heart_df = pd.read_csv('heart.csv', header=None)\n",
    "\n",
    "#Dropping the first row as it contains the column names\n",
    "heart_df = heart_df.drop(heart_df.index[0])\n",
    "\n",
    "#Heart data has 13 features and 1 output, the last column is the output, 0 means no heart disease and 1 means heart disease\n",
    "heart_data_outputs = heart_df.iloc[:, -1]\n",
    "\n",
    "#Converting the output to a one hot encoded vector\n",
    "heart_data_outputs = pd.get_dummies(heart_data_outputs)\n",
    "\n",
    "#Converting the dataframe to a numpy array\n",
    "heart_data_outputs = heart_data_outputs.to_numpy()\n",
    "\n",
    "#Dropping the last column from the dataframe since we have used it already in the output\n",
    "#Droping the last 4 columns as they are not required\n",
    "heart_df = heart_df.drop(heart_df.columns[[-1, -2, -3, -4]], axis=1)\n",
    "\n",
    "#Converting the dataframe to a numpy array\n",
    "heart_data_inputs = heart_df.to_numpy()\n",
    "\n",
    "#Ensuring all the remaining values are floats\n",
    "heart_data_inputs = heart_data_inputs.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING HEART DATA WITH MLP\n",
    "#Creating the MLP\n",
    "\n",
    "input_size = 10 #Number of features\n",
    "hidden_size = 8 #Number of hidden neurons\n",
    "output_size = 2 #Number of outputs\n",
    "\n",
    "heart_mlp = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "#Setting the training inputs and outputs\n",
    "heart_mlp.train_inputs = heart_data_inputs\n",
    "heart_mlp.train_outputs = heart_data_outputs\n",
    "\n",
    "xs = heart_data_inputs.T\n",
    "#heart_mlp.feedforward(xs)\n",
    "\n",
    "#Training the MLP\n",
    "epochs = 6000\n",
    "\n",
    "learning_rate = 0.08\n",
    "\n",
    "print(\"\\nOutputs after training:\")\n",
    "c = heart_mlp.train(epochs, learning_rate)\n",
    "print(\"cost = \", str(c[-1]))\n",
    "print(heart_mlp.feedforward(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING HEART DATA WITH MLP\n",
    "#Creating the MLP\n",
    "\n",
    "input_size = 10 #Number of features\n",
    "hidden_size = 8 #Number of hidden neurons\n",
    "output_size = 2 #Number of outputs\n",
    "\n",
    "heart_mlp = MLP_Cross_Entropy(input_size, hidden_size, output_size)\n",
    "\n",
    "#Setting the training inputs and outputs\n",
    "heart_mlp.train_inputs = heart_data_inputs\n",
    "heart_mlp.train_outputs = heart_data_outputs\n",
    "\n",
    "xs = heart_data_inputs.T\n",
    "#heart_mlp.feedforward(xs)\n",
    "\n",
    "#Training the MLP\n",
    "epochs = 8000\n",
    "\n",
    "learning_rate = 0.0075\n",
    "\n",
    "print(\"\\nOutputs after training:\")\n",
    "c = heart_mlp.train(epochs, learning_rate)\n",
    "print(\"cost = \", str(c[-1]))\n",
    "print(heart_mlp.feedforward(xs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
